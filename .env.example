# ===== OLIST E-COMMERCE DATA PIPELINE - ENVIRONMENT VARIABLES =====
# Copy this file to .env and fill in your actual values
# NEVER commit the actual .env file to version control!

# ===== SNOWFLAKE CREDENTIALS =====
# Snowflake Data Warehouse connection details
SNOWFLAKE_ACCOUNT=your_snowflake_account # Your Snowflake account identifier (e.g., xy12345.east-us-2.azure)
SNOWFLAKE_USER=your_snowflake_user # Your Snowflake username
SNOWFLAKE_PASSWORD=your_snowflake_password # Your Snowflake password
SNOWFLAKE_DATABASE=your_snowflake_database # The Snowflake database to use
SNOWFLAKE_WAREHOUSE=your_snowflake_warehouse # The Snowflake warehouse to use
SNOWFLAKE_ROLE=your_snowflake_role # The Snowflake role to use (e.g., ACCOUNTADMIN, SYSADMIN)
SNOWFLAKE_SCHEMA=your_snowflake_schema # The default schema to use in Snowflake

# ===== AIRFLOW CONFIGURATION =====
# Airflow settings for DAG execution
AIRFLOW_HOME=/opt/airflow # The Airflow home directory
AIRFLOW__CORE__DAGS_FOLDER=/opt/airflow/dags # The directory containing Airflow DAGs
AIRFLOW__CORE__EXECUTOR=SequentialExecutor # Or CeleryExecutor, LocalExecutor # The Airflow executor to use
AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow # Replace with your DB connection string # The connection string for the Airflow metadata database
AIRFLOW__CORE__FERNET_KEY= # Generate a Fernet key # The Fernet key for encrypting connections and variables

# ===== DBT CONFIGURATION =====
# dbt profile settings
DBT_PROFILES_DIR=/opt/airflow/dbt_profiles # Or your preferred directory # The directory containing dbt profiles.yml

# ===== OTHER ENVIRONMENT VARIABLES =====
PYTHON_VERSION=3.9 # The Python version used in the project